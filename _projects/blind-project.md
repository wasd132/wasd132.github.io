---
layout: project
title: "Blind Project - 시각장애인 보행 보조장치"
summary: "시각장애인을 위한 보행 보조장치입니다. YOLOv5 기반 실시간 객체 인식과 Android 스마트폰 앱, 소켓 통신을 활용하여 보행 경로 상의 위험 요소를 탐지하고 음성으로 경고를 제공합니다."
image: assets/images/mentors.jpg
technologies: [YOLOv5, CVAT, Python, Android Studio, Raspberry Pi, OpenCV, Socket, PyGame]
github: https://github.com/KunsanDADLab/BlindProject
categories: ml
---

## 프로젝트 배경

시각장애인은 독립적인 보행 시 다양한 위험 요소에 노출됩니다. 기존의 보행 보조장치(흰 지팡이, 안내견 등)는 한계가 있으며, 실시간 객체 인식 기술을 활용한 새로운 보조 시스템의 필요성이 대두되었습니다. 본 프로젝트는 **제9회 대한민국 SW 융복합 해커톤 대회**에 BRAINGYM팀으로 참가하여 개발한 시각장애인 보행 보조장치입니다.

### 보행 보조 시연
<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind2.mp4" type="video/mp4">
</video>

## 시스템 아키텍처

시스템은 **Android 스마트폰 앱(클라이언트)** 과 **서버(AI 추론)** 로 구성됩니다. 초기 프로토타입은 Raspberry Pi를 활용했으며, 최종본은 Android Studio로 개발한 스마트폰 앱으로 완성했습니다.

```
[Android 스마트폰 앱 (카메라)] → (TCP 소켓 통신) → [서버: YOLOv5 추론] → (결과 전송) → [앱: 음성 경고]
```
### 객체 탐지 데모
<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind1.mp4" type="video/mp4">
</video>

### 1. Android 스마트폰 앱 (최종 클라이언트)
- **Android Studio**로 개발한 스마트폰 앱
- 스마트폰 카메라로 실시간 영상 촬영
- JPEG 인코딩 후 TCP 소켓을 통해 서버로 이미지 전송
- 서버로부터 탐지 결과를 수신하여 음성 경고 출력

### 2. 서버 (YOLOv5 기반 객체 탐지)
- TCP 소켓 서버로 클라이언트에서 전송된 이미지를 수신
- 커스텀 학습된 YOLOv5 모델(`best_n_22.09.01.pt`)로 실시간 추론 수행
- 탐지 결과를 분석하여 위험도를 판별하고 클라이언트에 결과 전송

### 3. 통신 구조
- TCP/IP 소켓 기반 클라이언트-서버 통신 
- 이미지 데이터 크기를 먼저 전송한 후 실제 데이터를 전송하는 프로토콜 설계
- 추론 결과를 `results.txt` 파일에 저장 후 클라이언트로 재전송

## 주요 기능

### 실시간 객체 탐지
- YOLOv5를 활용하여 보행 경로 상의 위험 요소를 실시간 인식
- **긴급(Emergency) 객체**: 스쿠터, 오토바이, 자전거, 자동차, 버스
- **경고(Warning) 객체**: 나무 줄기, 스테인리스 볼라드, 대리석 볼라드, 고무 볼라드, 벤치, 사람, 배수구 덮개(grating)

### 진로 방해 판별 알고리즘
- 640×640 이미지를 1사분면 좌표계로 변환하여 분석
- Y좌표 기준선(245px / 337px)으로 근거리 객체만 필터링
- X좌표 320px(화면 중앙)을 기준으로 좌측/우측 판별
- **좌측 일차함수** `f(x) = (337/213) × x` 와 **우측 일차함수** `g(x) = (-337/214) × x + (107840/107)` 을 활용하여 보행 경로 침범 여부를 수학적으로 계산

<img src="{{ site.baseurl }}/assets/images/blind-path-detection.png" alt="진로 방해 판별 알고리즘 - 객체 탐지 결과 (scooter, bollard_rubber 탐지 및 보행 경로 영역 표시)" width="100%">

> 노란색 삼각형 영역이 보행 경로를 나타내며, 이 영역 안에 들어오는 객체만 경고 대상으로 판별합니다. 위 이미지에서는 고무 볼라드(bollard_rubber, 신뢰도 0.51)가 보행 경로 내에 위치하여 경고(Warning) 대상으로, 스쿠터(scooter, 신뢰도 0.69)는 경로 밖에 있어 필터링됩니다.

### 음성 경고 시스템 (유지보수 버전)
- PyGame mixer를 활용한 MP3 음성 안내
- 탐지된 객체 유형(컵, 핸드폰 등)과 방향(8방위: 북, 북동, 동, 남동, 남, 남서, 서, 북서)을 음성으로 안내
- 손의 위치를 기준으로 대상 객체의 상대적 방향을 9분면으로 계산

### 결과 전달 형식
- 탐지 결과는 `E*(긴급)` 또는 `W*(경고)` 접두사와 객체명으로 구성
- 여러 객체 탐지 시 `&`로 구분 (예: `E*scooter&W*bollard_stainless`)

## 기술 스택 상세

### YOLOv5
- 커스텀 데이터셋을 활용한 전이학습(Transfer Learning) 수행
- 모델 버전 이력: `detect_v1.py` → `detect_v2.py` → `detect_22.08.02.py` → `detect_22.08.09.py` → `server_09.17.py` (해커톤 최종)
- 추론 이미지 크기: 640×640, Confidence Threshold: 0.25, IoU Threshold: 0.45
- CUDA GPU 가속 지원

### CVAT (Computer Vision Annotation Tool)
- Docker 기반 CVAT 환경을 구축하여 팀 단위 데이터 라벨링 수행
- 바운딩 박스 어노테이션으로 학습 데이터 제작

### Android Studio (최종 클라이언트 앱)
- Java 기반 Android 앱 개발
- 스마트폰 카메라 API를 활용한 실시간 영상 캡처
- TCP 소켓 통신으로 서버와 이미지/결과 송수신
- TTS(Text-to-Speech) 등을 활용한 음성 경고 출력

### Raspberry Pi & 하드웨어 (초기 프로토타입)
- Raspberry Pi GPIO를 활용한 물리 버튼 입력 처리
- RGB LED 제어 (3색: 빨강/초록/파랑)
- OpenCV 기반 카메라 프레임 캡처 및 전처리

### 소켓 통신
- Python `socket` 모듈 기반 TCP 클라이언트-서버 구조
- Base64 인코딩/디코딩을 통한 이미지 데이터 전송
- 멀티스레딩 지원 설계

## 개발 타임라인

| 기간 | 활동 |
|------|------|
| 2022.01 | 프로젝트 기획 및 팀 회의 시작 |
| 2022.02 ~ 03 | 데이터 수집 및 라벨링, 모델 학습 |
| 2022.05 | 하드웨어 팀 - Raspberry Pi 통신 구현 (이미지 전송, LED, 메시지 핑퐁) |
| 2022.06 ~ 07 | 유지보수 - YOLOv5 탐지 알고리즘 고도화, 음성 안내 기능 개발 |
| 2022.08 | 해커톤 대회 참가, Android Studio 앱 개발 및 최종 서버 코드 완성 |
| 2022.09 | 최종 모델(`best_n_22.09.01.pt`) 적용 |
| 2022.10 | 프로젝트 정리 및 문서화 |

## 문제 해결 과정

### 1. 클라이언트 전환: Raspberry Pi → Android 스마트폰

**문제**: 초기 프로토타입은 Raspberry Pi에 카메라 모듈을 연결하고 GPIO 버튼으로 촬영하는 방식이었습니다. 그러나 실제 시각장애인이 사용하기에는 Raspberry Pi + 보조배터리 + 카메라 모듈을 별도로 휴대해야 하는 부담이 있었고, 하드웨어 구성이 복잡하여 실용성이 떨어졌습니다.

**해결**: 누구나 이미 가지고 있는 스마트폰으로 클라이언트를 전환하기로 결정했습니다. Android Studio를 사용하여 Java 기반 앱을 개발했고, 스마트폰의 내장 카메라, 스피커, 네트워크를 활용하면 별도의 하드웨어 없이도 동일한 기능을 구현할 수 있었습니다. 이를 통해 사용자는 앱 하나만 설치하면 바로 사용할 수 있게 되었습니다.

### 2. 통신 프로토콜 설계 및 안정화

**문제**: TCP 소켓으로 이미지를 전송할 때, 한 번에 보낼 수 있는 데이터 크기가 제한되어 있어 이미지가 잘리거나 깨지는 현상이 발생했습니다. 클라이언트에서 보낸 이미지가 서버에서 온전히 수신되지 않는 문제가 빈번했습니다.

**해결**: 이미지 데이터를 보내기 전에 **데이터 크기(length)를 먼저 전송**하는 프로토콜을 설계했습니다. 서버는 이 크기를 먼저 읽고, 정확히 해당 바이트 수만큼만 소켓에서 데이터를 수신하여 이미지를 복원합니다. `get_bytes_stream()` 함수에서 `remain` 변수로 남은 바이트를 추적하며 반복 수신하는 방식으로 데이터 유실 문제를 해결했습니다.

```python
# 서버 측 바이트 스트림 수신 코드
def get_bytes_stream(self, sock, length):
    buffer = b''
    remain = length
    while True:
        data = sock.recv(remain)
        buffer += data
        if len(buffer) == length:
            break
        elif len(buffer) < length:
            remain = length - len(buffer)
    return buffer[:length]
```

### 3. Raspberry Pi ↔ 서버 통신에서 Android ↔ 서버 통신으로의 전환

**문제**: 초기 Raspberry Pi 클라이언트는 Python 기반으로 `socket` + `base64` + `OpenCV`를 사용하여 이미지를 전송했습니다. Android 앱으로 전환하면서 Java의 소켓 API와 이미지 인코딩 방식이 달라 기존 서버와의 호환성 문제가 발생했습니다.

**해결**: 서버 측 수신 코드를 수정하여 Java(Android)에서 보내는 바이트 스트림 형식에 맞추었습니다. 기존 Python 클라이언트는 Base64 인코딩 후 문자열 길이를 64바이트로 패딩하여 전송했지만, Android 앱에서는 바이트 배열 앞에 길이 정보를 포함하는 방식으로 변경했습니다. 서버의 `receiveImages()` 메서드에서 `bytearray(self.conn.recv(1024))[2:]`로 앞 2바이트를 건너뛰고 길이를 읽도록 처리하여 Java와 Python 간의 데이터 형식 차이를 해결했습니다.

### 4. 진로 방해 판별 알고리즘 고도화

**문제**: 초기 버전(`detect_v1.py`, `detect_v2.py`)은 단순히 객체를 탐지하고 방향만 안내하는 수준이었습니다. 하지만 탐지된 모든 객체가 실제로 보행에 위협이 되는 것은 아니었고, 멀리 있는 객체나 보행 경로를 벗어난 객체까지 경고하면 사용자에게 불필요한 알림이 과도하게 발생하는 문제가 있었습니다.

**해결**: 좌표 기반 수학적 필터링 알고리즘을 설계했습니다. 640×640 이미지를 1사분면 좌표계로 변환한 뒤, Y좌표 기준선으로 근거리 객체만 필터링하고, X좌표 중앙(320px)을 기준으로 좌/우를 나눈 후 일차함수 `f(x)`와 `g(x)`를 적용하여 **실제 보행 경로를 침범하는 객체만** 경고하도록 개선했습니다. 또한 객체를 긴급(Emergency)과 경고(Warning)으로 분류하여 위험도에 따라 다른 알림을 제공했습니다.

### 5. 서버 연결 끊김 및 재연결 처리

**문제**: 네트워크 환경이 불안정한 경우 소켓 연결이 끊어지면 클라이언트와 서버 모두 프로그램이 종료되는 문제가 있었습니다.

**해결**: 예외 처리와 자동 재연결 로직을 구현했습니다. 소켓 통신 중 에러가 발생하면 `socketClose()` 후 `socketOpen()`을 호출하여 서버를 다시 대기 상태로 전환하고, 클라이언트 측에서도 `connectServer()`를 재귀적으로 호출하되 최대 재시도 횟수(10회)를 두어 무한 루프를 방지했습니다.

### 6. 데이터 어노테이션 환경 구축 및 라벨링 품질 확보

**문제**: YOLOv5 모델을 커스텀 객체(볼라드, 스쿠터, 배수구 등)에 대해 학습시키려면 대량의 라벨링된 이미지 데이터가 필요했습니다. 그러나 5명의 팀원이 각자 다른 환경에서 라벨링 작업을 진행하다 보니 다음과 같은 문제가 발생했습니다.

- **라벨링 기준 불일치**: 팀원마다 바운딩 박스를 그리는 기준이 달라 같은 객체도 어떤 사람은 여유 있게, 어떤 사람은 빡빡하게 박스를 그려 학습 데이터의 일관성이 떨어졌습니다.
- **클래스 분류 혼동**: 유사한 객체(예: 스테인리스 볼라드 vs 대리석 볼라드 vs 고무 볼라드)를 구분하는 기준이 모호하여 잘못 분류되는 경우가 있었습니다.
- **작업 중복 및 누락**: 누가 어떤 이미지를 라벨링했는지 관리가 되지 않아 동일 이미지를 중복 작업하거나, 누락되는 이미지가 발생했습니다.

**해결**: Docker 기반 **CVAT(Computer Vision Annotation Tool)** 서버를 구축하여 팀 전체가 하나의 환경에서 협업할 수 있도록 했습니다.

- **공유 어노테이션 환경**: CVAT 서버를 팀 내부 서버에 배포하여 모든 팀원이 웹 브라우저로 접속해 동일한 프로젝트에서 라벨링 작업을 수행하도록 통합했습니다. 이를 통해 작업 할당과 진행 상황을 한눈에 파악할 수 있었습니다.
- **라벨링 가이드라인 수립**: 각 클래스별로 바운딩 박스를 그리는 기준(객체 경계에 밀착, 가려진 부분은 추정하여 포함 등)을 문서화하고, CVAT의 라벨 설명에 예시 이미지를 첨부하여 팀원 간 기준을 통일했습니다.
- **교차 검수**: 한 사람이 라벨링한 결과를 다른 팀원이 CVAT에서 검토하는 교차 검수 프로세스를 도입하여 오분류 및 부정확한 바운딩 박스를 수정했습니다.
- **YOLO 포맷 내보내기**: CVAT에서 직접 YOLO 형식(클래스 번호 + 정규화된 xywh 좌표)으로 어노테이션을 내보낼 수 있어 별도의 형식 변환 작업 없이 바로 학습에 활용할 수 있었습니다.

## 성과

- **제9회 대한민국 SW 융복합 해커톤 대회** 전북도지사상 수상 (2022.08)
