---
layout: project
title: "Blind Project - 시각장애인 보행 보조장치"
summary: >
  시각장애인을 위한 보행 보조장치입니다. YOLOv5 기반 실시간 객체 인식과 Android 스마트폰 앱, 소켓 통신을 활용하여 보행 경로 상의 위험 요소를 탐지하고 음성으로 경고를 제공합니다.
image: "{{ site.baseurl }}/assets/images/mentors.jpg"
technologies: [YOLOv5, CVAT, Python, Android Studio, Raspberry Pi, OpenCV, Socket, PyGame]
github: https://github.com/KunsanDADLab/BlindProject
categories: ml
---

## 개요

시각장애인이 보행 시 직면하는 돌발적인 위험 요소(스쿠터, 볼라드 등)는 기존의 흰 지팡이나 안내견만으로는 완벽히 회피하기 어렵습니다. 이를 해결하기 위해 스마트폰 카메라로 전방을 촬영하고, 실시간 객체 인식(YOLOv5)을 통해 **사용자의 실제 보행 경로 상에 있는 위험 요소만을 선별하여 방향과 함께 음성으로 경고**하는 AI 기반 보행 보조 시스템을 개발했습니다.

***

## 주요 성과

- **대회 수상:** 제9회 대한민국 SW 융복합 해커톤 대회 **전북도지사상 수상**
- **실용성 확보:** 고가의 별도 하드웨어(라즈베리파이 등) 없이 **Android 스마트폰 앱만으로 동작**하는 접근성 높은 구조 완성
- **데이터 파이프라인 구축:** Docker 기반 **CVAT 중앙화 서버**를 구축하여 5인의 팀원이 단기간(해커톤) 내에 고품질 커스텀 데이터셋 라벨링 완료
- **알고리즘 최적화:** 단순 객체 탐지를 넘어, 1차 함수 기반의 **진로 방해 판별 알고리즘**을 도입하여 시각장애인에게 불필요한 알림 피로도(Noise)를 대폭 감소

***

### 보행 보조 시연
<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind2.mp4" type="video/mp4">
</video>

## 시스템 구조

```
[Android 스마트폰 앱 (카메라)] → (TCP 소켓 통신) → [서버: YOLOv5 추론] → (결과 전송) → [앱: 음성 경고]
```

- **Android 스마트폰 앱**: 카메라 촬영 → JPEG 인코딩 → TCP 소켓으로 서버 전송 → 탐지 결과 수신 → 음성 경고 출력
- **서버(YOLOv5)**: 커스텀 학습 모델로 실시간 추론, 긴급/경고 등급 분류, 진로 방해 판별 알고리즘 적용
- **객체 분류**: 긴급(Emergency) — 스쿠터, 오토바이, 자전거, 자동차, 버스 / 경고(Warning) — 볼라드, 벤치, 사람, 배수구 덮개 등

<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind1.mp4" type="video/mp4">
</video>

## 주요 기능

### 1. 커스텀 객체 탐지 및 위험도 분류 (YOLOv5)
시각장애인 보행 시 위협이 되는 12개 객체를 학습하여 실시간으로 탐지하고, 즉각적 회피 필요성에 따라 2단계 등급으로 분류하여 알림의 우선순위를 부여했습니다.
- **긴급 (Emergency):** 즉각적인 회피가 필요한 동적 이동체 (스쿠터, 오토바이, 자전거, 자동차, 버스)
- **경고 (Warning):** 주의가 필요한 보행 경로 상의 정적 장애물 (볼라드 3종, 벤치, 나무 줄기, 사람, 배수구 덮개)

### 2. 진로 방해 판별 알고리즘 (수학적 필터링)
화면에 보이는 모든 객체를 알림으로 주면 시각장애인에게 혼란(Noise)을 초래합니다. 이를 방지하기 위해 사용자의 실제 보행 궤적 안에 들어오는 객체만 필터링하는 로직을 구현했습니다.

<img src="{{ site.baseurl }}/assets/images/blind-path-detection.png" alt="진로 방해 판별 알고리즘" width="100%">

- 화면(640x640)을 1사분면 좌표계로 변환하고, Y좌표 기준선(245px, 337px)을 설정해 원거리 객체를 1차 제외했습니다.
- 화면 중앙(X=320px)을 기준으로 좌측 일차함수 `f(x)`와 우측 일차함수 `g(x)`를 정의하여, **두 함수가 그리는 가상의 삼각형(노란색 영역) 내부에 바운딩 박스가 위치하는 객체만 최종 경고 대상으로 판별**했습니다.

### 3. 접근성을 고려한 클라이언트 (Android + 음성 안내)
초기 라즈베리파이 프로토타입의 한계(휴대성 저하, 배터리 문제, 비용 부담)를 극복하기 위해 스마트폰 앱으로 전환했습니다.
- 카메라 API로 프레임을 캡처하고 TCP 소켓의 앞단에 데이터 크기(Length)를 명시하는 프로토콜을 적용해 이미지 유실을 방지했습니다.
- PyGame mixer와 TTS를 연동하여, 탐지된 객체의 종류와 8방위(북동, 남서 등) 상대적 위치를 음성으로 안내합니다.


## 프로젝트 관리 및 기술 의사결정 

짧은 해커톤 기간 내에 5명의 팀원이 협업하여 성공적으로 모델을 학습시키기 위한 PM(Project Manager) 역할과 데이터 파이프라인 관리에 집중했습니다.

### 1. 중앙화된 라벨링 환경 배포 (Docker + CVAT)
- **Situation (상황):** 5명의 팀원이 각자 로컬에서 라벨링을 진행할 경우, 어노테이션 포맷(YOLO vs COCO 등) 불일치, 작업 중복, 진행률 트래킹 불가 등 데이터 파편화 문제가 발생할 위기였습니다.
- **Action (행동):** 데이터의 일관성을 보장하기 위해 팀 내부 서버에 **Docker Compose를 활용하여 CVAT(Computer Vision Annotation Tool) 서버를 직접 배포**했습니다. 12개 클래스의 색상과 메타데이터를 사전 세팅하고, 라벨링 완료 시 즉시 YOLO 형식(클래스 번호 + 정규화 좌표)으로 Export되도록 파이프라인을 자동화했습니다.
- **Result (결과):** 팀원들은 별도 프로그램 설치 없이 브라우저만으로 라벨링에 참여할 수 있었으며, 포맷 변환 작업에 소요되는 시간을 0으로 단축해 모델 반복 학습 주기를 크게 앞당겼습니다.

### 2. 데이터 품질 관리를 위한 가이드라인 수립 및 교차 검수
- **Situation (상황):** 작업자마다 바운딩 박스를 그리는 기준(예: 볼라드의 그림자 포함 여부)이 다르고, 시각적으로 유사한 클래스(스테인리스 vs 대리석 볼라드) 간 오분류가 모델의 mAP(Mean Average Precision)를 저하시키는 원인이 되었습니다.
- **Action (행동):** 
  1. 객체의 재질과 형태 차이를 명시한 **사진 기반의 라벨링 가이드라인을 작성**하여 CVAT Description에 연동했습니다.
  2. 단순 작업 후 병합이 아닌 **'1차 라벨링 → 교차 검수(Peer Review) → 수정'의 3단계 프로세스**를 도입했습니다. 다른 팀원이 Task를 열어 오류(예: 객체 절반만 박스 처리, 고무 볼라드 오분류 등)를 직접 수정하거나 코멘트를 남기도록 강제했습니다.
- **Result (결과):** 교차 검수 도입 직후 라벨링 오류율이 급감했으며, 일관된 기준의 고품질 어노테이션 데이터를 바탕으로 커스텀 모델의 실제 환경 탐지 정확도를 목표치 이상으로 끌어올렸습니다.

### 3. 해커톤 병목 현상 해소를 위한 일정 관리
- **Situation (상황):** 해커톤 마감(D-7) 시점에 라벨링 완료율이 60%에 머물러, 모델 학습 및 통합 테스트 일정이 연쇄적으로 지연될 심각한 리스크가 발생했습니다.
- **Action (행동):** CVAT 대시보드를 통해 팀원별 Task 진행률(프레임 처리량)을 실시간 모니터링하여, 진척이 더딘 작업량을 타 팀원에게 즉각 재분배했습니다. 또한, 전체 라벨링 완료를 기다리지 않고 **모델 성능 체감에 가장 큰 영향을 미치는 핵심 클래스(스쿠터, 볼라드)가 포함된 Task를 최우선 순위로 끌어올려 선행 처리**하도록 지시했습니다.
- **Result (결과):** 라벨링 병목 현상을 타개하고 마감 전 핵심 클래스에 대한 학습(detect_v1 → server_09.17)을 여러 차례 반복할 수 있는 시간을 확보하여, 성공적으로 최종 통합 데모를 완성했습니다.