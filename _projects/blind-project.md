---
layout: project
title: "Blind Project - 시각장애인 보행 보조장치"
summary: "시각장애인을 위한 보행 보조장치입니다. YOLOv5 기반 실시간 객체 인식과 Android 스마트폰 앱, 소켓 통신을 활용하여 보행 경로 상의 위험 요소를 탐지하고 음성으로 경고를 제공합니다."
image: assets/images/mentors.jpg
technologies: [YOLOv5, CVAT, Python, Android Studio, Raspberry Pi, OpenCV, Socket, PyGame]
github: https://github.com/KunsanDADLab/BlindProject
categories: ml
---

## 프로젝트 배경

시각장애인은 독립적인 보행 시 다양한 위험 요소에 노출됩니다. 기존의 보행 보조장치(흰 지팡이, 안내견 등)는 한계가 있으며, 실시간 객체 인식 기술을 활용한 새로운 보조 시스템의 필요성이 대두되었습니다. 본 프로젝트는 **제9회 대한민국 SW 융복합 해커톤 대회**에 BRAINGYM팀(5명)으로 참가하여 개발한 시각장애인 보행 보조장치입니다.

### 보행 보조 시연
<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind2.mp4" type="video/mp4">
</video>

## 시스템 구조

```
[Android 스마트폰 앱 (카메라)] → (TCP 소켓 통신) → [서버: YOLOv5 추론] → (결과 전송) → [앱: 음성 경고]
```

- **Android 스마트폰 앱**: 카메라 촬영 → JPEG 인코딩 → TCP 소켓으로 서버 전송 → 탐지 결과 수신 → 음성 경고 출력
- **서버(YOLOv5)**: 커스텀 학습 모델로 실시간 추론, 긴급/경고 등급 분류, 진로 방해 판별 알고리즘 적용
- **객체 분류**: 긴급(Emergency) — 스쿠터, 오토바이, 자전거, 자동차, 버스 / 경고(Warning) — 볼라드, 벤치, 사람, 배수구 덮개 등

<video width="100%" controls>
  <source src="{{ site.baseurl }}/assets/videos/blind1.mp4" type="video/mp4">
</video>

## 담당 업무

5명으로 구성된 팀에서 저는 **객체 탐지 모델의 학습 환경 구축**, **라벨링 품질 관리**, **일정 관리**를 담당했습니다.

### 1. 학습 환경 구축: Docker 기반 CVAT 서버 배포

팀원 5명이 협업하여 YOLOv5 커스텀 모델을 학습시키려면, 대량의 이미지를 일관된 기준으로 라벨링할 수 있는 공유 환경이 필요했습니다. 각자 로컬에서 개별 도구를 사용하면 어노테이션 포맷 차이, 작업 중복, 진행 상황 파악 불가 등의 문제가 발생하기 때문에, **Docker 기반 CVAT(Computer Vision Annotation Tool) 서버를 팀 내부 서버에 직접 배포**하여 중앙화된 라벨링 환경을 구축했습니다.

- CVAT를 Docker Compose로 팀 서버에 배포하고, 팀원 5명 각각에게 계정을 생성하여 웹 브라우저만으로 접속·작업할 수 있도록 설정했습니다.
- CVAT 프로젝트 내에서 12개 탐지 클래스(scooter, motorcycle, bicycle, car, bus, tree_trunk, bollard_stainless, bollard_marble, bollard_rubber, bench, person, grating)를 사전 등록하고, 각 클래스별 라벨 색상을 지정하여 시각적으로 구분이 쉽도록 구성했습니다.
- CVAT에서 직접 **YOLO 형식(클래스 번호 + 정규화된 xywh 좌표)** 으로 어노테이션을 내보내도록 설정하여, 별도의 포맷 변환 없이 바로 YOLOv5 학습 파이프라인에 투입할 수 있게 했습니다.
- 학습 파라미터는 이미지 크기 640×640, Confidence Threshold 0.25, IoU Threshold 0.45로 설정하고, 모델 버전을 `detect_v1.py` → `detect_v2.py` → `detect_22.08.02.py` → `server_09.17.py`(해커톤 최종)로 반복 학습하며 성능을 개선했습니다.

### 2. 라벨링 품질 관리: 가이드라인 수립과 교차 검수

5명의 팀원이 동시에 라벨링을 수행하면서, 바운딩 박스 기준 불일치와 유사 클래스 간 오분류가 모델 정확도를 떨어뜨리는 핵심 원인임을 파악했습니다. 이를 해결하기 위해 **라벨링 가이드라인 문서를 직접 작성**하고, **교차 검수 프로세스를 설계·운영**했습니다.

**가이드라인 수립:**
- 각 클래스별로 바운딩 박스 그리는 기준을 문서화했습니다. 예를 들어, 볼라드의 경우 "기둥 전체를 포함하되 지면 접촉부까지 박스에 넣을 것", 사람의 경우 "가방·우산 등 부속물은 제외하고 신체만 포함할 것" 등 구체적 규칙을 정했습니다.
- 유사 클래스 간 혼동을 방지하기 위해 **실제 촬영 이미지에서 클래스별 비교 예시**를 제작했습니다. 스테인리스 볼라드(은색 금속 재질, 원통형), 대리석 볼라드(돌 재질, 사각/원형 상단), 고무 볼라드(주황/노란색, 유연한 재질)의 차이를 사진과 함께 정리하여 팀원들에게 공유했습니다.
- CVAT의 라벨 설명(description) 필드에 각 클래스별 판별 기준과 예시 이미지를 첨부하여, 라벨링 작업 중 언제든 참고할 수 있게 했습니다.

**교차 검수:**
- **1차 라벨링 → 교차 검수 → 수정** 3단계 프로세스를 도입했습니다. 한 팀원이 라벨링을 완료하면 다른 팀원이 CVAT에서 해당 Task를 열어 바운딩 박스의 위치·크기·클래스를 검토하고, 오류를 발견하면 직접 수정하거나 코멘트를 남기도록 했습니다.
- 검수 초기에 발견된 대표적 오류로는, 고무 볼라드를 스테인리스 볼라드로 분류한 경우(12건), 바운딩 박스가 객체의 절반만 포함한 경우(8건) 등이 있었으며, 이를 가이드라인에 반영하여 동일 오류 재발을 방지했습니다.
- 교차 검수 도입 전후로 라벨링 오류율이 체감상 크게 감소했고, 이후 학습된 모델의 탐지 정확도가 향상되는 결과로 이어졌습니다.

### 3. 일정 관리: 해커톤 데드라인에 맞춘 작업 분배

해커톤 대회 일정에 맞춰 데이터 수집 → 라벨링 → 모델 학습 → 통합 테스트까지의 전체 파이프라인을 기한 내에 완료해야 했습니다. 라벨링 작업이 병목이 되지 않도록 **CVAT의 Task 단위로 작업을 분배하고 진행 상황을 추적**했습니다.

- 전체 수집 이미지를 500장 단위의 Task로 나누어 CVAT에 업로드하고, 각 팀원에게 Task를 할당했습니다. CVAT 대시보드에서 Task별 진행률(완료/전체 프레임)을 실시간으로 확인하여, 진행이 느린 팀원에게는 작업량을 재분배했습니다.
- 해커톤 D-7 시점에 라벨링 완료율이 60%에 머물러 일정이 촉박해졌을 때, 남은 이미지 중 **모델 성능에 영향이 큰 클래스(스쿠터, 볼라드)** 를 우선적으로 라벨링하도록 우선순위를 조정하여, 핵심 클래스의 학습 데이터를 먼저 확보했습니다.
- 라벨링 완료와 동시에 YOLO 포맷으로 내보내어 바로 학습을 시작할 수 있도록, 라벨링 마감과 학습 시작 일정을 연동하여 파이프라인 전환 시간을 최소화했습니다.

## 주요 기능

### 1. 실시간 객체 탐지
YOLOv5 커스텀 학습 모델(`best_n_22.09.01.pt`)로 보행 경로 상의 위험 요소를 실시간으로 인식합니다. 탐지 대상은 위험도에 따라 두 등급으로 분류됩니다.
- **긴급(Emergency)**: 스쿠터, 오토바이, 자전거, 자동차, 버스 — 빠르게 접근하여 즉각적인 회피가 필요한 이동체
- **경고(Warning)**: 스테인리스/대리석/고무 볼라드, 나무 줄기, 벤치, 사람, 배수구 덮개(grating) — 보행 경로 상의 고정 장애물

추론 파라미터는 이미지 크기 640×640, Confidence Threshold 0.25, IoU Threshold 0.45로 설정하며, CUDA GPU 가속을 지원합니다. 탐지 결과는 `E*(긴급)` 또는 `W*(경고)` 접두사와 객체명으로 구성되고, 여러 객체 탐지 시 `&`로 구분합니다. (예: `E*scooter&W*bollard_stainless`)

### 2. 진로 방해 판별 알고리즘
탐지된 모든 객체가 실제로 보행에 위협이 되는 것은 아니므로, 불필요한 알림을 줄이기 위해 좌표 기반 수학적 필터링 알고리즘을 적용합니다.

1. 640×640 이미지를 **1사분면 좌표계**로 변환 (좌하단 원점)
2. Y좌표 기준선(245px / 337px)으로 **근거리 객체만 필터링** — 멀리 있는 객체는 제외
3. X좌표 320px(화면 중앙)을 기준으로 좌측/우측 판별
4. **좌측 일차함수** `f(x) = (337/213) × x` 와 **우측 일차함수** `g(x) = (-337/214) × x + (107840/107)` 을 적용하여, 두 함수가 만드는 삼각형 영역(보행 경로) 안에 들어오는 객체만 경고 대상으로 판별

<img src="{{ site.baseurl }}/assets/images/blind-path-detection.png" alt="진로 방해 판별 알고리즘" width="100%">

> 노란색 삼각형 영역이 보행 경로를 나타내며, 이 영역 안에 들어오는 객체만 경고 대상으로 판별합니다. 위 이미지에서는 고무 볼라드(bollard_rubber, 신뢰도 0.51)가 보행 경로 내에 위치하여 경고(Warning) 대상으로, 스쿠터(scooter, 신뢰도 0.69)는 경로 밖에 있어 필터링됩니다.

### 3. 음성 경고 시스템
탐지된 객체의 유형과 방향을 음성으로 안내하여, 시각 정보 없이도 주변 상황을 인지할 수 있도록 합니다.
- **PyGame mixer**를 활용한 MP3 음성 재생
- **8방위 방향 안내**: 북, 북동, 동, 남동, 남, 남서, 서, 북서로 객체의 상대적 위치를 안내
- 화면을 9분면으로 나누어 탐지된 객체의 바운딩 박스 중심 좌표로부터 방향을 계산
- 긴급(Emergency) 객체는 즉시 경고음과 함께 안내하고, 경고(Warning) 객체는 일반 안내 음성으로 구분하여 전달

### 4. Android 스마트폰 앱
초기 프로토타입은 Raspberry Pi + 보조배터리 + 카메라 모듈을 별도로 휴대해야 하여 실용성이 떨어졌습니다. 또한 시각 장애인들의 경제적 부담을 고려해야 했기 때문에, 이를 해결하기 위해 Android Studio(Java)로 클라이언트를 전환했습니다.
- 스마트폰 **카메라 API**로 실시간 영상 촬영 및 JPEG 인코딩
- **TCP 소켓 통신**으로 서버에 이미지 전송 — 데이터 크기(length)를 먼저 전송하는 프로토콜을 적용하여 이미지 유실 방지
- 서버로부터 탐지 결과를 수신하여 **TTS(Text-to-Speech)** 로 음성 경고 출력
- 네트워크 불안정 시 자동 재연결 로직 구현 (최대 10회 재시도)

## 기술 스택

- **YOLOv5 + CVAT** — 커스텀 데이터셋 전이학습, Docker 기반 CVAT로 팀 단위 데이터 라벨링 (640×640, Conf 0.25, IoU 0.45)
- **Android Studio (Java)** — 스마트폰 카메라 API, TCP 소켓 통신, TTS 음성 경고
- **Python + OpenCV + Socket** — TCP 서버, 이미지 수신/추론, Base64 인코딩
- **Raspberry Pi** — 초기 프로토타입 (GPIO 버튼, RGB LED, 카메라 모듈)

## 성과

- **제9회 대한민국 SW 융복합 해커톤 대회** 전북도지사상 수상 (2022.08)
